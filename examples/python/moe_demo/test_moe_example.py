#!/usr/bin/env python3
"""
æ¸¬è©¦ MoE ç¯„ä¾‹çš„è…³æœ¬

é€™å€‹è…³æœ¬æœƒæ¸¬è©¦ï¼š
1. å°ˆå®¶çµæ§‹æ˜¯å¦æ­£ç¢º
2. NiXL å‚³è¼¸æ˜¯å¦æ­£å¸¸å·¥ä½œ
3. è·¨ GPU è¨ˆç®—æ˜¯å¦æˆåŠŸ
"""

import sys
import os
import torch
import torch.nn.functional as F

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from moe_simple_example import Expert, init_nixl_agents, transfer_tensor


def test_expert_structure():
    """æ¸¬è©¦å°ˆå®¶çµæ§‹"""
    print("=== æ¸¬è©¦å°ˆå®¶çµæ§‹ ===")
    
    # å‰µå»ºå°ˆå®¶
    expert = Expert(input_dim=512, hidden_dim=2048, device="cuda:0")
    
    # æª¢æŸ¥æ¬Šé‡å½¢ç‹€ (é©é… F.linear çš„è¦æ±‚)
    assert expert.weight1.shape == (2048, 512), f"æ¬Šé‡1å½¢ç‹€éŒ¯èª¤ï¼š{expert.weight1.shape}"
    assert expert.weight2.shape == (512, 2048), f"æ¬Šé‡2å½¢ç‹€éŒ¯èª¤ï¼š{expert.weight2.shape}"
    
    # å‰µå»ºæ¸¬è©¦è¼¸å…¥
    test_input = torch.randn(1, 1, 512, device="cuda:0")
    
    # åŸ·è¡Œå‰å‘å‚³æ’­
    output = expert.forward(test_input)
    
    # æª¢æŸ¥è¼¸å‡ºå½¢ç‹€
    assert output.shape == (1, 1, 512), f"è¼¸å‡ºå½¢ç‹€éŒ¯èª¤ï¼š{output.shape}"
    assert output.device == torch.device("cuda:0"), f"è¼¸å‡ºè¨­å‚™éŒ¯èª¤ï¼š{output.device}"
    
    print("âœ“ å°ˆå®¶çµæ§‹æ¸¬è©¦é€šé")
    return True


def test_nixl_agents():
    """æ¸¬è©¦ NiXL agents åˆå§‹åŒ–"""
    print("=== æ¸¬è©¦ NiXL Agents ===")
    
    try:
        src_agent, dst_agent = init_nixl_agents()
        print("âœ“ NiXL agents åˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— NiXL agents åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        return False


def test_gpu_availability():
    """æ¸¬è©¦ GPU å¯ç”¨æ€§"""
    print("=== æ¸¬è©¦ GPU å¯ç”¨æ€§ ===")
    
    gpu_count = torch.cuda.device_count()
    print(f"å¯ç”¨ GPU æ•¸é‡ï¼š{gpu_count}")
    
    if gpu_count < 2:
        print("âœ— éœ€è¦è‡³å°‘ 2 å€‹ GPU ä¾†åŸ·è¡Œå®Œæ•´æ¸¬è©¦")
        return False
    
    # æª¢æŸ¥æ¯å€‹ GPU
    for i in range(gpu_count):
        device_name = torch.cuda.get_device_name(i)
        print(f"GPU {i}: {device_name}")
    
    print("âœ“ GPU å¯ç”¨æ€§æ¸¬è©¦é€šé")
    return True


def test_cross_gpu_transfer():
    """æ¸¬è©¦è·¨ GPU å‚³è¼¸"""
    print("=== æ¸¬è©¦è·¨ GPU å‚³è¼¸ ===")
    
    if torch.cuda.device_count() < 2:
        print("è·³éè·¨ GPU å‚³è¼¸æ¸¬è©¦ï¼ˆéœ€è¦è‡³å°‘ 2 å€‹ GPUï¼‰")
        return True
    
    try:
        # åˆå§‹åŒ– NiXL agents
        src_agent, dst_agent = init_nixl_agents()
        
        # å‰µå»ºæ¸¬è©¦å¼µé‡
        src_tensor = torch.randn(256, 256, device="cuda:0")
        dst_tensor = torch.zeros_like(src_tensor, device="cuda:1")
        
        # åŸ·è¡Œå‚³è¼¸
        success = transfer_tensor(
            src_agent, dst_agent,
            src_tensor, dst_tensor,
            "TEST_TRANSFER"
        )
        
        if success:
            print("âœ“ è·¨ GPU å‚³è¼¸æ¸¬è©¦é€šé")
            return True
        else:
            print("âœ— è·¨ GPU å‚³è¼¸æ¸¬è©¦å¤±æ•—")
            return False
            
    except Exception as e:
        print(f"âœ— è·¨ GPU å‚³è¼¸æ¸¬è©¦å¤±æ•—ï¼š{e}")
        return False


def test_expert_computation():
    """æ¸¬è©¦å°ˆå®¶è¨ˆç®—"""
    print("=== æ¸¬è©¦å°ˆå®¶è¨ˆç®— ===")
    
    try:
        # å‰µå»ºå°ˆå®¶
        expert = Expert(input_dim=512, hidden_dim=2048, device="cuda:0")
        
        # å‰µå»ºæ¸¬è©¦è¼¸å…¥
        test_input = torch.randn(1, 1, 512, device="cuda:0")
        
        # åŸ·è¡Œè¨ˆç®—
        output = expert.forward(test_input)
        
        # é©—è­‰è¼¸å‡º
        assert output.shape == test_input.shape, "è¼¸å‡ºå½¢ç‹€èˆ‡è¼¸å…¥ä¸åŒ¹é…"
        assert not torch.allclose(output, test_input), "è¼¸å‡ºèˆ‡è¼¸å…¥ç›¸åŒï¼Œå¯èƒ½è¨ˆç®—æœ‰å•é¡Œ"
        
        print("âœ“ å°ˆå®¶è¨ˆç®—æ¸¬è©¦é€šé")
        return True
        
    except Exception as e:
        print(f"âœ— å°ˆå®¶è¨ˆç®—æ¸¬è©¦å¤±æ•—ï¼š{e}")
        return False


def test_gelu_activation():
    """æ¸¬è©¦ GeLU æ¿€æ´»å‡½æ•¸"""
    print("=== æ¸¬è©¦ GeLU æ¿€æ´»å‡½æ•¸ ===")
    
    try:
        # å‰µå»ºæ¸¬è©¦è¼¸å…¥
        x = torch.randn(10, 10)
        
        # æ‰‹å‹•å¯¦ç¾ GeLU
        expected = x * 0.5 * (1 + torch.erf(x / torch.sqrt(torch.tensor(2.0))))
        
        # ä½¿ç”¨ PyTorch çš„ GeLU
        actual = F.gelu(x)
        
        # æ¯”è¼ƒçµæœ
        assert torch.allclose(expected, actual, atol=1e-6), "GeLU å¯¦ç¾ä¸æ­£ç¢º"
        
        print("âœ“ GeLU æ¿€æ´»å‡½æ•¸æ¸¬è©¦é€šé")
        return True
        
    except Exception as e:
        print(f"âœ— GeLU æ¿€æ´»å‡½æ•¸æ¸¬è©¦å¤±æ•—ï¼š{e}")
        return False


def run_all_tests():
    """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
    print("é–‹å§‹åŸ·è¡Œ MoE ç¯„ä¾‹æ¸¬è©¦...")
    print("=" * 50)
    
    tests = [
        ("GPU å¯ç”¨æ€§", test_gpu_availability),
        ("å°ˆå®¶çµæ§‹", test_expert_structure),
        ("GeLU æ¿€æ´»å‡½æ•¸", test_gelu_activation),
        ("å°ˆå®¶è¨ˆç®—", test_expert_computation),
        ("NiXL Agents", test_nixl_agents),
        ("è·¨ GPU å‚³è¼¸", test_cross_gpu_transfer),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nåŸ·è¡Œæ¸¬è©¦ï¼š{test_name}")
        try:
            if test_func():
                passed += 1
            else:
                print(f"æ¸¬è©¦å¤±æ•—ï¼š{test_name}")
        except Exception as e:
            print(f"æ¸¬è©¦ç•°å¸¸ï¼š{test_name} - {e}")
    
    print("\n" + "=" * 50)
    print(f"æ¸¬è©¦çµæœï¼š{passed}/{total} é€šé")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼MoE ç¯„ä¾‹å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒè¨­å®šã€‚")
        return False


def main():
    """ä¸»å‡½æ•¸"""
    print("MoE ç¯„ä¾‹æ¸¬è©¦è…³æœ¬")
    print("=" * 50)
    
    # æª¢æŸ¥ PyTorch å’Œ CUDA
    print(f"PyTorch ç‰ˆæœ¬ï¼š{torch.__version__}")
    print(f"CUDA å¯ç”¨ï¼š{torch.cuda.is_available()}")
    print(f"CUDA ç‰ˆæœ¬ï¼š{torch.version.cuda}")
    
    if torch.cuda.is_available():
        print(f"GPU æ•¸é‡ï¼š{torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}ï¼š{torch.cuda.get_device_name(i)}")
    else:
        print("è­¦å‘Šï¼šCUDA ä¸å¯ç”¨ï¼ŒæŸäº›æ¸¬è©¦å°‡è¢«è·³é")
    
    print()
    
    # åŸ·è¡Œæ¸¬è©¦
    success = run_all_tests()
    
    if success:
        print("\nâœ… æ¸¬è©¦å®Œæˆï¼Œç¯„ä¾‹å¯ä»¥æ­£å¸¸ä½¿ç”¨")
        sys.exit(0)
    else:
        print("\nâŒ æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒè¨­å®š")
        sys.exit(1)


if __name__ == "__main__":
    main()
