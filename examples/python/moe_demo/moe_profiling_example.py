#!/usr/bin/env python3
"""
MoE Profiling Example for Nsight Systems/Nsight Compute

é€™å€‹è…³æœ¬å°ˆé–€è¨­è¨ˆç”¨æ–¼ Nsight profilingï¼ŒåŒ…å«ï¼š
1. GPU è¨ˆç®— profiling
2. GPU é–“æ•¸æ“šå‚³è¼¸ profiling  
3. GPU-CPU æ•¸æ“šå‚³è¼¸ profiling
4. è©³ç´°çš„ profiling markers å’Œ annotations

ä½¿ç”¨æ–¹å¼ï¼š
1. Nsight Systems: nsys profile python moe_profiling_example.py
2. Nsight Compute: ncu python moe_profiling_example.py
"""

import os
import sys
import time
import torch
import torch.nn.functional as F
import torch.profiler
import numpy as np

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from moe_simple_example import Expert, init_nixl_agents, transfer_tensor

# æª¢æŸ¥æ˜¯å¦æœ‰ NVTX (NVIDIA Tools Extension) æ”¯æ´
try:
    import nvtx
    NVTX_AVAILABLE = True
    print("âœ“ NVTX å¯ç”¨æ–¼ profiling annotations")
except ImportError:
    NVTX_AVAILABLE = False
    print("âš ï¸  NVTX ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨åŸºæœ¬çš„ profiling")


def nvtx_range(name):
    """NVTX ç¯„åœè£é£¾å™¨ï¼Œå¦‚æœ NVTX å¯ç”¨å‰‡ä½¿ç”¨ï¼Œå¦å‰‡ä½¿ç”¨ç©ºæ“ä½œ"""
    def decorator(func):
        if NVTX_AVAILABLE:
            def wrapper(*args, **kwargs):
                nvtx.range_push(name)
                try:
                    result = func(*args, **kwargs)
                finally:
                    nvtx.range_pop()
                return result
            return wrapper
        else:
            return func
    return decorator


class ProfilingExpert(Expert):
    """å¸¶æœ‰è©³ç´° profiling æ¨™è¨˜çš„å°ˆå®¶å¯¦ç¾"""
    
    @nvtx_range("Expert_Forward")
    def forward(self, x):
        """å‰å‘å‚³æ’­ï¼ŒåŒ…å«è©³ç´°çš„ profiling æ¨™è¨˜"""
        
        # æ¨™è¨˜ç¬¬ä¸€å€‹ç·šæ€§è®Šæ›
        if NVTX_AVAILABLE:
            nvtx.range_push("Linear1_Computation")
        
        hidden = F.linear(x, self.weight1)
        torch.cuda.synchronize()  # ç¢ºä¿è¨ˆç®—å®Œæˆ
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        # æ¨™è¨˜ GeLU æ¿€æ´»
        if NVTX_AVAILABLE:
            nvtx.range_push("GeLU_Activation")
        
        hidden = F.gelu(hidden)
        torch.cuda.synchronize()
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        # æ¨™è¨˜ç¬¬äºŒå€‹ç·šæ€§è®Šæ›
        if NVTX_AVAILABLE:
            nvtx.range_push("Linear2_Computation")
        
        output = F.linear(hidden, self.weight2)
        torch.cuda.synchronize()
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        return output


@nvtx_range("GPU_Memory_Operations")
def profile_gpu_memory_operations():
    """æ¸¬è©¦å’Œ profile GPU è¨˜æ†¶é«”æ“ä½œ"""
    print("\n=== GPU è¨˜æ†¶é«”æ“ä½œ Profiling ===")
    
    # æ¸¬è©¦ä¸åŒå¤§å°çš„è¨˜æ†¶é«”åˆ†é…
    sizes = [
        (256, 256),      # å°å¼µé‡
        (1024, 1024),    # ä¸­ç­‰å¼µé‡
        (2048, 2048),    # å¤§å¼µé‡
    ]
    
    for size in sizes:
        if NVTX_AVAILABLE:
            nvtx.range_push(f"Memory_Alloc_{size[0]}x{size[1]}")
        
        # GPU è¨˜æ†¶é«”åˆ†é…
        tensor = torch.randn(size, device="cuda:0")
        torch.cuda.synchronize()
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        print(f"åˆ†é… GPU å¼µé‡ï¼š{size}")


@nvtx_range("GPU_to_GPU_Transfer")
def profile_gpu_to_gpu_transfer():
    """æ¸¬è©¦å’Œ profile GPU é–“æ•¸æ“šå‚³è¼¸"""
    print("\n=== GPU é–“æ•¸æ“šå‚³è¼¸ Profiling ===")
    
    if torch.cuda.device_count() < 2:
        print("éœ€è¦è‡³å°‘ 2 å€‹ GPU ä¾†æ¸¬è©¦ GPU é–“å‚³è¼¸")
        return
    
    # åˆå§‹åŒ– NiXL agents
    src_agent, dst_agent = init_nixl_agents()
    
    # æ¸¬è©¦ä¸åŒå¤§å°çš„å¼µé‡å‚³è¼¸
    transfer_sizes = [
        (512, 512),      # 1MB
        (1024, 1024),    # 4MB  
        (2048, 2048),    # 16MB
    ]
    
    for size in transfer_sizes:
        if NVTX_AVAILABLE:
            nvtx.range_push(f"GPU_Transfer_{size[0]}x{size[1]}")
        
        # å‰µå»ºæºå¼µé‡å’Œç›®æ¨™å¼µé‡
        src_tensor = torch.randn(size, device="cuda:0")
        dst_tensor = torch.zeros(size, device="cuda:1")
        
        # åŸ·è¡Œ NiXL å‚³è¼¸
        success = transfer_tensor(
            src_agent, dst_agent,
            src_tensor, dst_tensor,
            f"PROFILE_TRANSFER_{size[0]}x{size[1]}"
        )
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        print(f"GPU é–“å‚³è¼¸ {size}: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        # åŒæ™‚æ¸¬è©¦æ¨™æº– PyTorch çš„ GPU é–“è¤‡è£½
        if NVTX_AVAILABLE:
            nvtx.range_push(f"PyTorch_GPU_Copy_{size[0]}x{size[1]}")
        
        pytorch_dst = src_tensor.to("cuda:1")
        torch.cuda.synchronize()
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()


@nvtx_range("GPU_to_CPU_Transfer")
def profile_gpu_to_cpu_transfer():
    """æ¸¬è©¦å’Œ profile GPU-CPU æ•¸æ“šå‚³è¼¸"""
    print("\n=== GPU-CPU æ•¸æ“šå‚³è¼¸ Profiling ===")
    
    # æ¸¬è©¦ä¸åŒå¤§å°çš„ GPU åˆ° CPU å‚³è¼¸
    transfer_sizes = [
        (256, 256),      # 256KB
        (1024, 1024),    # 4MB
        (2048, 2048),    # 16MB
    ]
    
    for size in transfer_sizes:
        # GPU åˆ° CPU
        if NVTX_AVAILABLE:
            nvtx.range_push(f"GPU_to_CPU_{size[0]}x{size[1]}")
        
        gpu_tensor = torch.randn(size, device="cuda:0")
        cpu_tensor = gpu_tensor.cpu()
        torch.cuda.synchronize()
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        # CPU åˆ° GPU
        if NVTX_AVAILABLE:
            nvtx.range_push(f"CPU_to_GPU_{size[0]}x{size[1]}")
        
        gpu_tensor_back = cpu_tensor.cuda()
        torch.cuda.synchronize()
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        print(f"GPU-CPU å‚³è¼¸ {size}: å®Œæˆ")


@nvtx_range("Expert_Computation_Profiling")
def profile_expert_computation():
    """æ¸¬è©¦å’Œ profile å°ˆå®¶è¨ˆç®—"""
    print("\n=== å°ˆå®¶è¨ˆç®— Profiling ===")
    
    # å‰µå»ºå°ˆå®¶
    expert_gpu0 = ProfilingExpert(input_dim=512, hidden_dim=2048, device="cuda:0")
    
    if torch.cuda.device_count() >= 2:
        expert_gpu1 = ProfilingExpert(input_dim=512, hidden_dim=2048, device="cuda:1")
    
    # æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°çš„è¨ˆç®—
    batch_sizes = [1, 8, 32, 128]
    sequence_length = 1
    hidden_dim = 512
    
    for batch_size in batch_sizes:
        if NVTX_AVAILABLE:
            nvtx.range_push(f"Expert_Computation_Batch_{batch_size}")
        
        # å‰µå»ºè¼¸å…¥å¼µé‡
        input_tensor = torch.randn(batch_size, sequence_length, hidden_dim, device="cuda:0")
        
        # GPU 0 ä¸Šçš„å°ˆå®¶è¨ˆç®—
        if NVTX_AVAILABLE:
            nvtx.range_push(f"Expert_GPU0_Batch_{batch_size}")
        
        output_gpu0 = expert_gpu0.forward(input_tensor)
        torch.cuda.synchronize()
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        # å¦‚æœæœ‰ç¬¬äºŒå€‹ GPUï¼Œåœ¨ GPU 1 ä¸Šé€²è¡Œè¨ˆç®—
        if torch.cuda.device_count() >= 2:
            if NVTX_AVAILABLE:
                nvtx.range_push(f"Expert_GPU1_Batch_{batch_size}")
            
            input_gpu1 = input_tensor.to("cuda:1")
            output_gpu1 = expert_gpu1.forward(input_gpu1)
            torch.cuda.synchronize()
            
            if NVTX_AVAILABLE:
                nvtx.range_pop()
        
        if NVTX_AVAILABLE:
            nvtx.range_pop()
        
        print(f"å°ˆå®¶è¨ˆç®— (batch_size={batch_size}): å®Œæˆ")


@nvtx_range("Complete_MoE_Workflow")
def profile_complete_moe_workflow():
    """æ¸¬è©¦å’Œ profile å®Œæ•´çš„ MoE å·¥ä½œæµç¨‹"""
    print("\n=== å®Œæ•´ MoE å·¥ä½œæµç¨‹ Profiling ===")
    
    if torch.cuda.device_count() < 2:
        print("éœ€è¦è‡³å°‘ 2 å€‹ GPU ä¾†åŸ·è¡Œå®Œæ•´çš„ MoE å·¥ä½œæµç¨‹")
        return
    
    # åˆå§‹åŒ–çµ„ä»¶
    src_agent, dst_agent = init_nixl_agents()
    expert_e1 = ProfilingExpert(input_dim=512, hidden_dim=2048, device="cuda:0")
    expert_e2 = ProfilingExpert(input_dim=512, hidden_dim=2048, device="cuda:1")
    
    # å‰µå»ºè¼¸å…¥ token
    token = torch.randn(1, 1, 512, device="cuda:0")
    
    # æ­¥é©Ÿ 1: Token å‚³è¼¸åˆ° GPU 1
    if NVTX_AVAILABLE:
        nvtx.range_push("MoE_Token_Transfer")
    
    dst_tensor = torch.zeros_like(token, device="cuda:1")
    success = transfer_tensor(
        src_agent, dst_agent,
        token, dst_tensor,
        "MOE_WORKFLOW_TOKEN"
    )
    
    if NVTX_AVAILABLE:
        nvtx.range_pop()
    
    if not success:
        print("Token å‚³è¼¸å¤±æ•—")
        return
    
    # æ­¥é©Ÿ 2: åœ¨ GPU 1 åŸ·è¡Œå°ˆå®¶è¨ˆç®—
    if NVTX_AVAILABLE:
        nvtx.range_push("MoE_Expert_Computation")
    
    result = expert_e2.forward(dst_tensor)
    
    if NVTX_AVAILABLE:
        nvtx.range_pop()
    
    # æ­¥é©Ÿ 3: çµæœå‚³è¼¸å› GPU 0
    if NVTX_AVAILABLE:
        nvtx.range_push("MoE_Result_Transfer")
    
    final_tensor = torch.zeros_like(result, device="cuda:0")
    success = transfer_tensor(
        src_agent, dst_agent,
        result, final_tensor,
        "MOE_WORKFLOW_RESULT"
    )
    
    if NVTX_AVAILABLE:
        nvtx.range_pop()
    
    if success:
        print("å®Œæ•´ MoE å·¥ä½œæµç¨‹: æˆåŠŸ")
    else:
        print("å®Œæ•´ MoE å·¥ä½œæµç¨‹: å¤±æ•—")


def main():
    """ä¸»å‡½æ•¸ï¼ŒåŸ·è¡Œæ‰€æœ‰ profiling æ¸¬è©¦"""
    print("=== MoE Profiling Example ===")
    print("å°ˆç‚º Nsight Systems/Compute profiling è¨­è¨ˆ")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    print(f"GPU æ•¸é‡: {torch.cuda.device_count()}")
    
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    
    print()
    
    # ç†±èº«é‹è¡Œï¼ˆé¿å…åˆå§‹åŒ–é–‹éŠ·å½±éŸ¿ profilingï¼‰
    if NVTX_AVAILABLE:
        nvtx.range_push("Warmup")
    
    warmup_tensor = torch.randn(100, 100, device="cuda:0")
    warmup_result = torch.mm(warmup_tensor, warmup_tensor)
    torch.cuda.synchronize()
    
    if NVTX_AVAILABLE:
        nvtx.range_pop()
    
    print("ç†±èº«å®Œæˆ\n")
    
    # åŸ·è¡Œå„ç¨® profiling æ¸¬è©¦
    try:
        profile_gpu_memory_operations()
        profile_gpu_to_cpu_transfer()
        profile_expert_computation()
        
        if torch.cuda.device_count() >= 2:
            profile_gpu_to_gpu_transfer()
            profile_complete_moe_workflow()
        else:
            print("\nâš ï¸  è·³é GPU é–“å‚³è¼¸æ¸¬è©¦ï¼ˆéœ€è¦è‡³å°‘ 2 å€‹ GPUï¼‰")
        
        print("\nğŸ‰ æ‰€æœ‰ profiling æ¸¬è©¦å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ Profiling æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    main()
